version: '3.8'

services:
  # Main application service (runs SessionManager, Scheduler, etc.)
  session-manager: # Renamed from coordinator
    build: .
    container_name: spotify-session-manager
    environment:
      - NODE_ENV=production
      - ROLE=session_manager # Clarify role if app uses it
      - SESSION_COUNT=50 # As per TECHNICAL_IMPLEMENTATION.md
      - PROXY_PROVIDER=smartproxy # As per TECHNICAL_IMPLEMENTATION.md
      - CAPTCHA_API_KEY=${CAPTCHA_API_KEY}
      # Add other necessary ENV VARS like API keys for SmartProxy if fetched via API
      # SMARTPROXY_API_USER=${SMARTPROXY_API_USER}
      # SMARTPROXY_API_PASS=${SMARTPROXY_API_PASS}
    volumes:
      - ./config:/app/config:ro
      - ./logs:/app/logs
      - ./sessions:/app/sessions
    ports:
      - "3000:3000" # Main app port (where dashboard runs if integrated)
    networks:
      - spotify-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"] # Updated to /health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Scalable session workers
  worker: # Renamed from session
    build: .
    container_name: spotify-worker # To avoid issues if not using dynamic naming
    environment:
      - NODE_ENV=production
      - ROLE=worker # Identifies as a session worker
      - WORKER_MODE=true # As per TECHNICAL_IMPLEMENTATION.md
      - DISPLAY=:99
      - CAPTCHA_API_KEY=${CAPTCHA_API_KEY}
      # SMARTPROXY_API_USER=${SMARTPROXY_API_USER} # Workers might also need proxy access
      # SMARTPROXY_API_PASS=${SMARTPROXY_API_PASS}
    volumes:
      - ./config:/app/config:ro
      - ./logs:/app/logs # Consider if workers should log to separate files or central logging
      - ./sessions:/app/sessions # Workers might need to save session data/cookies
      - /dev/shm:/dev/shm
    networks:
      - spotify-network
    restart: unless-stopped
    depends_on:
      session-manager: # Depends on the main app service
        condition: service_healthy
    # No healthcheck for workers by default, as they don't typically run a server.
    # Their health can be inferred by task completion or via session-manager.
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SYS_ADMIN # Review if still needed with --no-sandbox in Playwright args

  # Separate Dashboard Service
  dashboard:
    build: . # Assuming it uses the same Dockerfile but runs in DASHBOARD_ONLY mode
    container_name: spotify-dashboard
    ports:
      - "8081:8080" # Exposing on host 8081, container listens on 8080
    environment:
      - NODE_ENV=production
      - DASHBOARD_ONLY=true # Application needs to recognize this
      - APP_PORT=8080 # Tell the app inside to use this port for the dashboard
      # It might need to know where session-manager API is if it fetches data
      # SESSION_MANAGER_API_URL=http://session-manager:3000/api
    volumes:
      - ./config:/app/config:ro # Read-only config
      - ./logs:/app/logs # Read-only logs if it displays log summaries
    restart: unless-stopped
    depends_on:
      session-manager:
        condition: service_healthy
    healthcheck: # Dashboard should also have a health check
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"] # Assumes dashboard runs its own /health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:v2.47.2 # Using a specific version
    container_name: spotify-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    restart: unless-stopped
    networks:
      - spotify-network
    depends_on:
      - session-manager # Or whichever service exposes /metrics

  # Grafana for metrics visualization
  grafana:
    image: grafana/grafana:10.1.5 # Using a specific version
    container_name: spotify-grafana
    ports:
      - "3001:3000" # Grafana default internal port is 3000
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin # Default admin password, change in production
      # Example: Configure Prometheus datasource
      # - GF_DATASOURCES_0_NAME=Prometheus
      # - GF_DATASOURCES_0_TYPE=prometheus
      # - GF_DATASOURCES_0_URL=http://prometheus:9090
      # - GF_DATASOURCES_0_ACCESS=proxy
      # - GF_DATASOURCES_0_IS_DEFAULT=true
    volumes:
      - grafana-storage:/var/lib/grafana
    restart: unless-stopped
    networks:
      - spotify-network
    depends_on:
      - prometheus

  # Optional: Redis for session coordination (if needed for large scale)
  redis:
    image: redis:7-alpine
    container_name: spotify-redis
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - spotify-network
    restart: unless-stopped
    profiles:
      - redis

networks:
  spotify-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16 # Example subnet

volumes:
  redis-data:
    driver: local
  grafana-storage: {} # Define the Grafana storage volume

# Example usage commands:
# docker-compose up -d session-manager worker dashboard prometheus grafana
#
# Scale workers:
#   docker-compose up -d --scale worker=10
#
# Start with Redis:
#   docker-compose --profile redis up -d
#
# View logs (e.g., session-manager or worker):
#   docker-compose logs -f session-manager
#
# Stop all:
#   docker-compose down
